from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage
import os
from difflib import SequenceMatcher
import csv
from pathlib import Path
from typing import Dict, Any, List, Optional

from kenopi_prompt import (
    KENOPI_SYSTEM_PROMPT, 
    KENOPI_THINKING_PROMPT,
    EMOTION_RESPONSIVE_PROMPT,
    QUALITY_ASSURANCE_PROMPT
)

# Sequential Thinking MCP í†µí•©
try:
    from sequential_thinking_mcp import thinking_mcp
    THINKING_AVAILABLE = True
    print("[Kenopi Chatbot] ğŸ§  Sequential Thinking MCP loaded successfully")
except ImportError as e:
    print(f"[Kenopi Chatbot] âš ï¸ Sequential Thinking not available: {e}")
    THINKING_AVAILABLE = False

# Map custom LANGSMITH_* env vars to LangChain expected keys
if os.getenv("LANGSMITH_API_KEY") and not os.getenv("LANGCHAIN_API_KEY"):
    os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGSMITH_API_KEY")
if os.getenv("LANGSMITH_ENDPOINT") and not os.getenv("LANGCHAIN_ENDPOINT"):
    os.environ["LANGCHAIN_ENDPOINT"] = os.getenv("LANGSMITH_ENDPOINT")
# Handle tracing flag (ì‚¬ìš©ì ì˜¤íƒ€ í¬í•¨)
tracing_flag = os.getenv("LANGSMITH_TRACING_V2") or os.getenv("LANGWSMITH_TRACING_V2")
if tracing_flag and not os.getenv("LANGCHAIN_TRACING_V2"):
    os.environ["LANGCHAIN_TRACING_V2"] = tracing_flag

project_name_env = os.getenv("LANGSMITH_PROJECT", "kenopi-cs")

# Enable LangSmith tracing via environment variables
if os.getenv("LANGCHAIN_TRACING_V2") == "true":
    print(f"[Kenopi Chatbot] LangSmith tracing enabled for project: {project_name_env}")
else:
    print("[Kenopi Chatbot] LangSmith tracing disabled")

# LangChain ì„¤ì • (OpenAI API í‚¤ê°€ ìˆì„ ë•Œë§Œ)
try:
    if os.getenv("OPENAI_API_KEY"):
        llm = ChatOpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            model="gpt-4o",
            temperature=0.3,
        )
        print("INFO: GPT-4o ëª¨ë¸ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        llm = None
        print("WARNING: OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. FAQ ì „ìš© ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
except Exception as e:
    llm = None
    print(f"WARNING: OpenAI ì„¤ì • ì‹¤íŒ¨: {e}. FAQ ì „ìš© ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

# Load FAQ dataset at import time
FAQ_PATH = Path(__file__).parent / "data" / "kenopi_faq.csv"
FAQ_LIST = []
if FAQ_PATH.exists():
    with FAQ_PATH.open("r", encoding="utf-8") as f:
        reader = csv.reader(f)
        next(reader)  # í—¤ë” ìŠ¤í‚µ
        for row in reader:
            if len(row) >= 3 and row[1] and row[2]:  # ì²« ë²ˆì§¸ëŠ” ë²ˆí˜¸, ë‘ ë²ˆì§¸ëŠ” ì§ˆë¬¸, ì„¸ ë²ˆì§¸ëŠ” ë‹µë³€
                FAQ_LIST.append({"question": row[1], "answer": row[2]})

SIM_THRESHOLD = 0.5

def _search_faq(query: str):
    """FAQì—ì„œ ìœ ì‚¬í•œ ì§ˆë¬¸ì„ ì°¾ì•„ ë‹µë³€ ë°˜í™˜ - ì •í™•í•œ ë§¤ì¹­ë§Œ"""
    best = None
    best_score = 0
    for item in FAQ_LIST:
        score = SequenceMatcher(None, query.lower(), item["question"].lower()).ratio()
        if score > best_score:
            best_score = score
            best = item
    
    # ì •í™•í•œ ë§¤ì¹­ë§Œ í—ˆìš© (0.8 ì´ìƒ)
    if best_score >= 0.8:
        return {"answer": best["answer"], "question": best["question"], "score": best_score}
    return None

def _find_intent_match(query: str):
    """ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•´ì„œ FAQ ì£¼ì œì™€ ë§¤ì¹­"""
    query_lower = query.lower()
    
    # ì˜ë„ë³„ í‚¤ì›Œë“œ ë§¤í•‘
    intent_keywords = {
        "í™˜ë¶ˆ": ["í™˜ë¶ˆ", "ëˆ", "ëŒë ¤", "ì·¨ì†Œ", "ì•ˆë°›", "ë°˜ë‚©"],
        "êµí™˜": ["êµí™˜", "ë°”ê¾¸", "ë‹¤ë¥¸ê±¸ë¡œ", "ì‚¬ì´ì¦ˆ", "ìƒ‰ê¹”"],
        "ë°˜í’ˆ": ["ë°˜í’ˆ", "ë³´ë‚´", "ëŒë ¤ë³´ë‚´", "ì•ˆë°›", "ì·¨ì†Œ"],
        "ë°°ì†¡ë¹„": ["ë°°ì†¡ë¹„", "íƒë°°ë¹„", "ë¹„ìš©", "ì–¼ë§ˆ", "ê°€ê²©"],
        "ê³ ê°ì„¼í„°": ["ì—°ë½", "ì „í™”", "ë¬¸ì˜", "ê³ ê°ì„¼í„°", "ìƒë‹´"],
        "ìŠ¤í¬ë˜ì¹˜": ["ìŠ¤í¬ë˜ì¹˜", "ê¸í˜", "ìƒì²˜", "í ì§‘"],
        "ììˆ˜": ["ììˆ˜", "ë¡œê³ ", "ë¸Œëœë“œ"],
        "ë¬¼ìƒ˜": ["ë¬¼", "ìƒˆ", "ë¹„", "ë°©ìˆ˜"],
        "ëƒ„ìƒˆ": ["ëƒ„ìƒˆ", "í–¥", "ëƒ„"],
        "ìŠ¤íŠ¸ë©": ["ìŠ¤íŠ¸ë©", "ëˆ", "ê³ ë¦¬", "ì—°ê²°"],
        "ê¸¸ì´ì¡°ì ˆ": ["ê¸¸ì´", "ì¡°ì ˆ", "ëŠ˜ë¦¬", "ì¤„ì´"],
        "ë°°ì†¡": ["ë°°ì†¡", "ì–¸ì œ", "ì¶œë°œ", "ë„ì°©"],
        "ì£¼ë¬¸í™•ì¸": ["ì£¼ë¬¸", "í™•ì¸", "ë‚´ì—­"],
        "AS": ["AS", "í’ˆì§ˆ", "ë³´ì¦", "í•˜ì"],
        "ë¸Œëœë“œ": ["ì¼€ë…¸í”¼", "ë¸Œëœë“œ", "íšŒì‚¬"],
        "ëŒ€ëŸ‰ì£¼ë¬¸": ["ëŒ€ëŸ‰", "ê¸°ì—…", "ë§ì´"],
        "í•´ì™¸ë°°ì†¡": ["í•´ì™¸", "ì™¸êµ­", "êµ­ì œ"]
    }
    
    # ê° ì˜ë„ë³„ë¡œ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
    intent_scores = {}
    for intent, keywords in intent_keywords.items():
        score = 0
        for keyword in keywords:
            if keyword in query_lower:
                score += 1
        if score > 0:
            intent_scores[intent] = score
    
    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì˜ë„ ë°˜í™˜
    if intent_scores:
        best_intent = max(intent_scores.items(), key=lambda x: x[1])
        return best_intent[0]
    
    return None

def generate_response(history: list[dict[str, str]], auto_mode: bool = True) -> str:
    """
    ì¼€ë…¸í”¼ CS ì±—ë´‡ ì‘ë‹µ ìƒì„± - ì˜ë„ íŒŒì•… ë° í™•ì¸ ì‹œìŠ¤í…œ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
    history: [{role: 'user'|'bot', content: str}, ...]
    auto_mode: ìë™ ëª¨ë“œ ì„ íƒ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    """
    if not history:
        return "ì•ˆë…•í•˜ì„¸ìš”! ì¼€ë…¸í”¼ ê³ ê°ì§€ì›íŒ€ ë…¸í”¼ğŸ¤–ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    
    latest_query = history[-1]["content"]
    
    # ğŸ¯ 1ë‹¨ê³„: ì •í™•í•œ FAQ ë§¤ì¹­ ì‹œë„
    faq_result = _search_faq(latest_query)
    if faq_result:
        return f"ì•ˆë…•í•˜ì„¸ìš”! ë…¸í”¼ğŸ¤–ì…ë‹ˆë‹¤. ğŸ˜Š\n\n{faq_result['answer']}"
    
    # âœ… 2ë‹¨ê³„: í™•ì¸ ì‘ë‹µ ì²˜ë¦¬ (ë„¤/ì˜ˆ/ë§ì•„ìš” ë“±)
    if _is_confirmation(latest_query) and len(history) >= 2:
        previous_bot_message = None
        for i in range(len(history) - 2, -1, -1):
            if history[i]["role"] == "bot":
                previous_bot_message = history[i]["content"]
                break
        
        if previous_bot_message:
            intent = _extract_intent_from_confirmation(previous_bot_message)
            if intent:
                faq_answer = _get_faq_by_intent(intent)
                if faq_answer:
                    return f"ë„¤, ì•Œë ¤ë“œë¦´ê²Œìš”! ğŸ˜Š\n\n{faq_answer}"
    
    # ğŸ¤” 3ë‹¨ê³„: ì˜ë„ íŒŒì•… ë° í™•ì¸ ì§ˆë¬¸
    intent = _find_intent_match(latest_query)
    if intent:
        return _get_confirmation_question(intent, latest_query)
    
    # ğŸš« 4ë‹¨ê³„: ì˜ë„ë„ íŒŒì•… ì•ˆë˜ë©´ ì •ì¤‘í•˜ê²Œ ê±°ì ˆ
    return _get_rejection_response(latest_query)

def _is_confirmation(query: str) -> bool:
    """ì‚¬ìš©ì ì‘ë‹µì´ í™•ì¸(ê¸ì •) ì‘ë‹µì¸ì§€ íŒë‹¨"""
    query_lower = query.lower().strip()
    positive_responses = [
        "ë„¤", "ì˜ˆ", "ë§ì•„ìš”", "ë§ìŠµë‹ˆë‹¤", "ê·¸ë ‡ìŠµë‹ˆë‹¤", "ë§ë‹¤", "ì‘", "ì–´", "ê·¸ë˜", "ê·¸ë ‡ë‹¤",
        "yes", "y", "ok", "okay", "ì¢‹ì•„", "ì¢‹ìŠµë‹ˆë‹¤", "ì•Œë ¤ì¤˜", "ì•Œë ¤ì£¼ì„¸ìš”", "ê¶ê¸ˆí•´", "ê¶ê¸ˆí•©ë‹ˆë‹¤"
    ]
    return query_lower in positive_responses

def _extract_intent_from_confirmation(bot_message: str) -> str:
    """ë´‡ì˜ í™•ì¸ ì§ˆë¬¸ì—ì„œ ì˜ë„ ì¶”ì¶œ"""
    if "í™˜ë¶ˆ ì •ì±…ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "í™˜ë¶ˆ"
    elif "êµí™˜ ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "êµí™˜"
    elif "ë°˜í’ˆ ë°©ë²•ì´ë‚˜ ì£¼ì†Œê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "ë°˜í’ˆ"
    elif "ë°°ì†¡ë¹„ê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "ë°°ì†¡ë¹„"
    elif "ê³ ê°ì„¼í„° ì—°ë½ì²˜ê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "ê³ ê°ì„¼í„°"
    elif "ìŠ¤í¬ë˜ì¹˜ ê´€ë ¨ ì •ì±…ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "ìŠ¤í¬ë˜ì¹˜"
    elif "ììˆ˜ ë¡œê³  ë¶€ë¶„ ê´€ë ¨ ë¬¸ì˜ì¸ê°€ìš”?" in bot_message:
        return "ììˆ˜"
    elif "ë°©ìˆ˜ ê´€ë ¨ ë¬¸ì˜ì¸ê°€ìš”?" in bot_message:
        return "ë¬¼ìƒ˜"
    elif "ëƒ„ìƒˆ ê´€ë ¨ ë¬¸ì˜ì¸ê°€ìš”?" in bot_message:
        return "ëƒ„ìƒˆ"
    elif "ìŠ¤íŠ¸ë©/í‚¤ë§ ì—°ê²° ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "ìŠ¤íŠ¸ë©"
    elif "ê¸¸ì´ ì¡°ì ˆì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "ê¸¸ì´ì¡°ì ˆ"
    elif "ë°°ì†¡ ì¼ì •ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "ë°°ì†¡"
    elif "ì£¼ë¬¸ í™•ì¸ ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "ì£¼ë¬¸í™•ì¸"
    elif "ASë‚˜ í’ˆì§ˆë³´ì¦ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "AS"
    elif "ë¸Œëœë“œ ì •ë³´ê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "ë¸Œëœë“œ"
    elif "ëŒ€ëŸ‰ ì£¼ë¬¸ì´ë‚˜ ê¸°ì—… êµ¬ë§¤ê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "ëŒ€ëŸ‰ì£¼ë¬¸"
    elif "í•´ì™¸ ë°°ì†¡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?" in bot_message:
        return "í•´ì™¸ë°°ì†¡"
    return None

def _get_faq_by_intent(intent: str) -> str:
    """ì˜ë„ì— ë”°ë¥¸ FAQ ë‹µë³€ ë°˜í™˜"""
    intent_to_faq_keywords = {
        "í™˜ë¶ˆ": ["í™˜ë¶ˆ", "ëˆ"],
        "êµí™˜": ["êµí™˜"],
        "ë°˜í’ˆ": ["ë°˜í’ˆ", "ì£¼ì†Œ"],
        "ë°°ì†¡ë¹„": ["ë°°ì†¡ë¹„"],
        "ê³ ê°ì„¼í„°": ["ê³ ê°ì„¼í„°", "ì—°ë½ì²˜"],
        "ìŠ¤í¬ë˜ì¹˜": ["ìŠ¤í¬ë˜ì¹˜"],
        "ììˆ˜": ["ììˆ˜"],
        "ë¬¼ìƒ˜": ["ë°©ìˆ˜", "ë¬¼"],
        "ëƒ„ìƒˆ": ["ëƒ„ìƒˆ"],
        "ìŠ¤íŠ¸ë©": ["ìŠ¤íŠ¸ë©", "í‚¤ë§"],
        "ê¸¸ì´ì¡°ì ˆ": ["ê¸¸ì´"],
        "ë°°ì†¡": ["ë°°ì†¡", "ì–¸ì œ"],
        "ì£¼ë¬¸í™•ì¸": ["ì£¼ë¬¸", "í™•ì¸"],
        "AS": ["AS", "í’ˆì§ˆ"],
        "ë¸Œëœë“œ": ["ì¼€ë…¸í”¼", "ë¸Œëœë“œ"],
        "ëŒ€ëŸ‰ì£¼ë¬¸": ["ëŒ€ëŸ‰", "ê¸°ì—…"],
        "í•´ì™¸ë°°ì†¡": ["í•´ì™¸"]
    }
    
    keywords = intent_to_faq_keywords.get(intent, [])
    
    # FAQì—ì„œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì§ˆë¬¸ ì°¾ê¸°
    for item in FAQ_LIST:
        for keyword in keywords:
            if keyword in item["question"]:
                return item["answer"]
    
    return None

def _get_confirmation_question(intent: str, original_query: str) -> str:
    """ì˜ë„ì— ë”°ë¥¸ í™•ì¸ ì§ˆë¬¸ ìƒì„±"""
    confirmation_questions = {
        "í™˜ë¶ˆ": "í™˜ë¶ˆ ì •ì±…ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "êµí™˜": "êµí™˜ ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "ë°˜í’ˆ": "ë°˜í’ˆ ë°©ë²•ì´ë‚˜ ì£¼ì†Œê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "ë°°ì†¡ë¹„": "ë°˜í’ˆ/êµí™˜ ì‹œ ë°°ì†¡ë¹„ê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "ê³ ê°ì„¼í„°": "ê³ ê°ì„¼í„° ì—°ë½ì²˜ê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "ìŠ¤í¬ë˜ì¹˜": "ì œí’ˆ ìŠ¤í¬ë˜ì¹˜ ê´€ë ¨ ì •ì±…ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "ììˆ˜": "ììˆ˜ ë¡œê³  ë¶€ë¶„ ê´€ë ¨ ë¬¸ì˜ì¸ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "ë¬¼ìƒ˜": "ìš°ì‚° ë°©ìˆ˜ ê´€ë ¨ ë¬¸ì˜ì¸ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "ëƒ„ìƒˆ": "ì œí’ˆ ëƒ„ìƒˆ ê´€ë ¨ ë¬¸ì˜ì¸ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "ìŠ¤íŠ¸ë©": "ìŠ¤íŠ¸ë©/í‚¤ë§ ì—°ê²° ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "ê¸¸ì´ì¡°ì ˆ": "ìŠ¤íŠ¸ë© ê¸¸ì´ ì¡°ì ˆì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "ë°°ì†¡": "ë°°ì†¡ ì¼ì •ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "ì£¼ë¬¸í™•ì¸": "ì£¼ë¬¸ í™•ì¸ ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "AS": "ASë‚˜ í’ˆì§ˆë³´ì¦ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "ë¸Œëœë“œ": "ì¼€ë…¸í”¼ ë¸Œëœë“œ ì •ë³´ê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "ëŒ€ëŸ‰ì£¼ë¬¸": "ëŒ€ëŸ‰ ì£¼ë¬¸ì´ë‚˜ ê¸°ì—… êµ¬ë§¤ê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)",
        "í•´ì™¸ë°°ì†¡": "í•´ì™¸ ë°°ì†¡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ë„¤/ì˜ˆ ë¼ê³  ë‹µí•´ì£¼ì‹œë©´ ìì„¸íˆ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”!)"
    }
    
    return confirmation_questions.get(intent, 
        "ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•˜ì‹ ì§€ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì‹œë©´ ë„ì™€ë“œë¦´ê²Œìš”! ğŸ˜Š")

def _get_rejection_response(user_question: str) -> str:
    """FAQì— ì—†ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ì •ì¤‘í•œ ê±°ì ˆ ì‘ë‹µ"""
    return (
        "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ëŠ” í˜„ì¬ ì œê³µí•´ë“œë¦¬ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤.\n\n"
        "ì¼€ë…¸í”¼ ê³ ê°ì„¼í„°(010-2747-9567, í‰ì¼ 10ì‹œ-17ì‹œ)ë¡œ ë¬¸ì˜í•´ì£¼ì‹œë©´ "
        "ì •í™•í•œ ë‹µë³€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

def _generate_auto_response(history: List[Dict[str, str]]) -> str:
    """ìë™ ëª¨ë“œ ì„ íƒì„ í†µí•œ ì‘ë‹µ ìƒì„±"""
    try:
        latest_query = history[-1]["content"]
        
        # ì§ˆë¬¸ ë³µì¡ë„ ë° ìœ í˜• ë¶„ì„
        complexity_analysis = _analyze_query_complexity_detailed(latest_query)
        complexity = complexity_analysis["complexity"]
        question_type = complexity_analysis["type"]
        urgency = complexity_analysis["urgency"]
        
        # FAQ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        faq_answer = _search_faq(latest_query)
        conversation_context = _build_conversation_context(history)
        
        # ìë™ ëª¨ë“œ ì„ íƒ ë¡œì§
        selected_mode = _select_optimal_mode(complexity, question_type, urgency, bool(faq_answer))
        
        print(f"[Auto Mode] ì§ˆë¬¸: '{latest_query[:50]}...' | ë³µì¡ë„: {complexity} | ì„ íƒëœ ëª¨ë“œ: {selected_mode}")
        
        # ì„ íƒëœ ëª¨ë“œì— ë”°ë¥¸ ì‘ë‹µ ìƒì„±
        if selected_mode == "basic":
            return _generate_basic_response(history)
        elif selected_mode == "thinking":
            return _generate_thinking_response_with_mode(history, "thinking")
        elif selected_mode == "enhanced":
            return _generate_thinking_response_with_mode(history, "enhanced")
        else:
            return _generate_basic_response(history)
            
    except Exception as e:
        print(f"[Auto Response Error] {e}")
        return _generate_basic_response(history)

def _analyze_query_complexity_detailed(query: str) -> Dict[str, Any]:
    """ìƒì„¸í•œ ì§ˆë¬¸ ë³µì¡ë„ ë° ìœ í˜• ë¶„ì„"""
    
    # ë³µì¡ë„ ì§€í‘œë“¤
    complexity_indicators = {
        "high": [
            "ì–´ë–»ê²Œ", "ì™œ", "ì´ìœ ", "ë°©ë²•", "ì ˆì°¨", "ë‹¨ê³„", "ê³¼ì •", 
            "ë¹„êµí•´", "ì°¨ì´", "ì¥ë‹¨ì ", "ë¬¸ì œí•´ê²°", "ë¶ˆëŸ‰", "ê³ ì¥",
            "í™˜ë¶ˆ", "êµí™˜", "ë°˜í’ˆ", "AS", "ìˆ˜ë¦¬", "ë³´ìƒ", "ë°°ìƒ"
        ],
        "medium": [
            "ì–¸ì œ", "ì–´ë””ì„œ", "ì–¼ë§ˆ", "ê°€ê²©", "ë¹„ìš©", "ê¸°ê°„", "ì‹œê°„",
            "ì •ì±…", "ê·œì •", "ì¡°ê±´", "ë°©ë²•", "ì•ˆë‚´", "ì„¤ëª…"
        ],
        "low": [
            "ì•ˆë…•", "ê°ì‚¬", "ë„¤", "ì˜ˆ", "ì•„ë‹ˆì˜¤", "í™•ì¸", "ì•Œë ¤ì£¼ì„¸ìš”",
            "ë¬¸ì˜", "ì—°ë½ì²˜", "ì „í™”ë²ˆí˜¸"
        ]
    }
    
    # ê¸´ê¸‰ë„ ì§€í‘œ
    urgency_indicators = {
        "high": ["ê¸´ê¸‰", "ê¸‰í•´", "ë¹¨ë¦¬", "ì¦‰ì‹œ", "ë‹¹ì¥", "ì§€ê¸ˆ", "ë¬¸ì œ", "ê³ ì¥", "ë¶ˆëŸ‰"],
        "medium": ["ì˜¤ëŠ˜", "ì´ë²ˆì£¼", "ë¹ ë¥¸", "ê°€ëŠ¥í•œ"],
        "low": ["ì–¸ì œ", "ë‚˜ì¤‘ì—", "ì—¬ìœ "]
    }
    
    # ì§ˆë¬¸ ìœ í˜• ë¶„ì„
    question_types = {
        "complaint": ["ë¶ˆë§Œ", "í™”", "ì§œì¦", "ë¬¸ì œ", "ë¶ˆëŸ‰", "ê³ ì¥", "ì˜ëª»"],
        "inquiry": ["ë¬¸ì˜", "ê¶ê¸ˆ", "ì•Œê³ ì‹¶", "í™•ì¸", "ì •ë³´"],
        "request": ["ìš”ì²­", "ë¶€íƒ", "ë„ì›€", "ì²˜ë¦¬", "í•´ê²°"],
        "greeting": ["ì•ˆë…•", "ì²˜ìŒ", "ë°˜ê°€", "ê°ì‚¬"]
    }
    
    query_lower = query.lower()
    
    # ë³µì¡ë„ ê³„ì‚°
    high_count = sum(1 for indicator in complexity_indicators["high"] if indicator in query_lower)
    medium_count = sum(1 for indicator in complexity_indicators["medium"] if indicator in query_lower)
    low_count = sum(1 for indicator in complexity_indicators["low"] if indicator in query_lower)
    
    # ê¸¸ì´ ê¸°ë°˜ ë³µì¡ë„ ì¡°ì •
    length_factor = len(query)
    
    if high_count >= 2 or (high_count >= 1 and length_factor > 30):
        complexity = "high"
    elif high_count >= 1 or medium_count >= 2 or length_factor > 50:
        complexity = "medium"
    elif low_count >= 1 and length_factor < 20:
        complexity = "low"
    else:
        complexity = "medium"  # ê¸°ë³¸ê°’
    
    # ê¸´ê¸‰ë„ ë¶„ì„
    urgency_high = sum(1 for indicator in urgency_indicators["high"] if indicator in query_lower)
    urgency_medium = sum(1 for indicator in urgency_indicators["medium"] if indicator in query_lower)
    
    if urgency_high >= 1:
        urgency = "high"
    elif urgency_medium >= 1:
        urgency = "medium"
    else:
        urgency = "low"
    
    # ì§ˆë¬¸ ìœ í˜• ë¶„ì„
    question_type = "general"
    for q_type, indicators in question_types.items():
        if any(indicator in query_lower for indicator in indicators):
            question_type = q_type
            break
    
    return {
        "complexity": complexity,
        "type": question_type,
        "urgency": urgency,
        "length": length_factor,
        "indicators": {
            "high": high_count,
            "medium": medium_count,
            "low": low_count
        }
    }

def _select_optimal_mode(complexity: str, question_type: str, urgency: str, has_faq: bool) -> str:
    """ìµœì  ëª¨ë“œ ìë™ ì„ íƒ"""
    
    # ì¸ì‚¬ë§ì´ë‚˜ ê°„ë‹¨í•œ í™•ì¸ ì§ˆë¬¸
    if question_type == "greeting" and complexity == "low":
        return "basic"
    
    # FAQê°€ ìˆëŠ” ê°„ë‹¨í•œ ì§ˆë¬¸
    if has_faq and complexity == "low":
        return "basic"
    
    # ë¶ˆë§Œì´ë‚˜ ê¸´ê¸‰í•œ ìƒí™©
    if question_type == "complaint" or urgency == "high":
        return "enhanced"  # ê°€ì¥ ì‹ ì¤‘í•œ ëª¨ë“œ
    
    # ë³µì¡ë„ ê¸°ë°˜ ì„ íƒ
    if complexity == "high":
        return "enhanced"
    elif complexity == "medium":
        return "thinking"
    else:
        return "basic"

def _generate_thinking_response_with_mode(history: List[Dict[str, str]], mode: str) -> str:
    """ì§€ì •ëœ ëª¨ë“œë¡œ Sequential Thinking ì‘ë‹µ ìƒì„±"""
    try:
        latest_query = history[-1]["content"]
        
        # FAQ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        faq_answer = _search_faq(latest_query)
        conversation_context = _build_conversation_context(history)
        
        # ëª¨ë“œë³„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if mode == "enhanced":
            context = f"""
{KENOPI_THINKING_PROMPT}

{EMOTION_RESPONSIVE_PROMPT}

{QUALITY_ASSURANCE_PROMPT}

**ğŸ” ê³ ê¸‰ ë¶„ì„ ëª¨ë“œ í™œì„±í™”**
- ë³µì¡í•œ ë¬¸ì˜ë‚˜ ì¤‘ìš”í•œ ìƒí™©ìœ¼ë¡œ íŒë‹¨ë¨
- ë‹¤ê°ë„ ê²€í†  ë° ìµœì  ì†”ë£¨ì…˜ ì œì‹œ
- ê³ ê° ë§Œì¡±ë„ ìµœìš°ì„  ê³ ë ¤

ëŒ€í™” íˆìŠ¤í† ë¦¬:
{conversation_context}

FAQ ë§¤ì¹­ ê²°ê³¼:
{faq_answer if faq_answer else "ë§¤ì¹­ë˜ëŠ” FAQ ì—†ìŒ"}
            """.strip()
        else:  # thinking mode
            context = f"""
{KENOPI_THINKING_PROMPT}

{EMOTION_RESPONSIVE_PROMPT}

**ğŸ§  ë‹¨ê³„ë³„ ì¶”ë¡  ëª¨ë“œ í™œì„±í™”**
- ì²´ê³„ì ì¸ ë¶„ì„ì„ í†µí•œ ì •í™•í•œ ë‹µë³€
- ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì • ì ìš©

ëŒ€í™” íˆìŠ¤í† ë¦¬:
{conversation_context}

FAQ ë§¤ì¹­ ê²°ê³¼:
{faq_answer if faq_answer else "ë§¤ì¹­ë˜ëŠ” FAQ ì—†ìŒ"}
            """.strip()
        
        # MCP Sequential Thinking ë¶„ì„ ë° ì‘ë‹µ
        result = thinking_mcp.analyze_and_respond(latest_query, context)
        
        if result["thinking_used"]:
            response = result["response"]
            # í’ˆì§ˆ ê²€ì¦
            if _validate_response_quality(response, latest_query):
                return _enhance_response_with_mode_info(response, mode)
            else:
                return _generate_basic_response(history)
        else:
            # Fallback to basic response
            return _generate_basic_response(history)
            
    except Exception as e:
        print(f"[Thinking Response Error] {e}")
        return _generate_basic_response(history)

def _enhance_response_with_mode_info(response: str, mode: str) -> str:
    """ì‘ë‹µì— ì„ íƒëœ ëª¨ë“œ ì •ë³´ ì¶”ê°€"""
    
    mode_info = {
        "basic": "ğŸ’¬ ê¸°ë³¸ ëª¨ë“œ",
        "thinking": "ğŸ§  ì¶”ë¡  ëª¨ë“œ", 
        "enhanced": "âš¡ ê³ ê¸‰ ëª¨ë“œ"
    }
    
    # ì‘ë‹µì´ ì¶©ë¶„íˆ ê¸¸ë©´ ëª¨ë“œ ì •ë³´ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ„í•´)
    if len(response) > 200:
        return response
    
    # ê°„ë‹¨í•œ ì‘ë‹µì¸ ê²½ìš° ëª¨ë“œ ì •ë³´ ì¶”ê°€
    if "ê³ ê°ì„¼í„°" not in response and len(response) < 150:
        response += f"\n\nğŸ’¡ **ì¶”ê°€ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´:**\nì¼€ë…¸í”¼ ê³ ê°ì„¼í„° ğŸ“ 1588-1234 (í‰ì¼ 9ì‹œ-18ì‹œ)\në” ê¶ê¸ˆí•œ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!"
    
    return response

def _generate_basic_response(history: List[Dict[str, str]]) -> str:
    """ê¸°ì¡´ ë°©ì‹ì˜ ê¸°ë³¸ ì‘ë‹µ ìƒì„± (Fallback)"""
    if not llm:
        # OpenAIê°€ ì—†ìœ¼ë©´ FAQ ê¸°ë°˜ ì‘ë‹µ
        if history:
            latest_query = history[-1]["content"]
            faq_result = _search_faq(latest_query)
            if faq_result:
                return f"ì•ˆë…•í•˜ì„¸ìš”! ë…¸í”¼ğŸ¤–ì…ë‹ˆë‹¤. ğŸ˜Š\n\n{faq_result['answer']}"
        
        return _get_rejection_response(history[-1]["content"] if history else "")
    
    messages = [SystemMessage(content=KENOPI_SYSTEM_PROMPT)]
    
    for m in history:
        if m["role"] == "user":
            messages.append(HumanMessage(content=m["content"]))
        else:
            messages.append(AIMessage(content=m["content"]))

    # FAQ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    if history:
        faq_answer = _search_faq(history[-1]["content"])
        if faq_answer:
            messages.insert(1, SystemMessage(content=f"FAQ ì°¸ê³  ë‹µë³€:\n{faq_answer}"))

    # LLM í˜¸ì¶œ ë° ì‘ë‹µ ìƒì„±
    answer = llm.invoke(messages)
    return answer.content

def _build_conversation_context(history: List[Dict[str, str]]) -> str:
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±"""
    if not history:
        return "ì²« ë²ˆì§¸ ë¬¸ì˜ì…ë‹ˆë‹¤."
    
    context_parts = []
    # ìµœê·¼ 4ê°œ ëŒ€í™”ë§Œ í¬í•¨ (ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ì œí•œ)
    recent_history = history[-8:] if len(history) > 8 else history
    
    for i, msg in enumerate(recent_history):
        role = "ê³ ê°" if msg["role"] == "user" else "ì¼€ë…¸í”¼"
        context_parts.append(f"{role}: {msg['content']}")
    
    return "\n".join(context_parts)

def _validate_response_quality(response: str, query: str) -> bool:
    """ì‘ë‹µ í’ˆì§ˆ ê¸°ë³¸ ê²€ì¦"""
    try:
        # ê¸°ë³¸ í’ˆì§ˆ ì²´í¬
        if not response or len(response.strip()) < 10:
            return False
        
        # ì¼€ë…¸í”¼ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€
        kenopi_indicators = ["ì¼€ë…¸í”¼", "ê³ ê°", "ë„ì›€", "ì•ˆë‚´", "ë¬¸ì˜", "ì„œë¹„ìŠ¤"]
        has_context = any(indicator in response for indicator in kenopi_indicators)
        
        # ì ì ˆí•œ ê¸¸ì´ (ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¸¸ì§€ ì•ŠìŒ)
        length_ok = 20 <= len(response) <= 1000
        
        return has_context and length_ok
        
    except Exception:
        return False

# ê³ ê¸‰ ì‘ë‹µ í•¨ìˆ˜ë„ ìë™ ëª¨ë“œ ì„ íƒ ì§€ì›
def generate_advanced_response(history: list[dict[str, str]]) -> Dict[str, Any]:
    """
    ìë™ ëª¨ë“œ ì„ íƒ ê¸°ë°˜ ê³ ê¸‰ ì‘ë‹µ ìƒì„± (ë¶„ì„ ì •ë³´ í¬í•¨)
    """
    if not history:
        return {
            "response": "ì•ˆë…•í•˜ì„¸ìš”! ì¼€ë…¸í”¼ AI ê³ ê°ì§€ì›íŒ€ì…ë‹ˆë‹¤. ğŸ§  ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¼ ìë™ìœ¼ë¡œ ìµœì ì˜ ë°©ì‹ìœ¼ë¡œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            "selected_mode": "auto",
            "complexity": "low",
            "quality_score": "high",
            "auto_selection": True
        }
    
    if not THINKING_AVAILABLE:
        return {
            "response": _generate_basic_response(history),
            "selected_mode": "basic",
            "complexity": "unknown",
            "quality_score": "basic",
            "auto_selection": False,
            "note": "Sequential Thinking ë¹„í™œì„±í™”"
        }
    
    try:
        latest_query = history[-1]["content"]
        
        # ìƒì„¸ ë¶„ì„
        complexity_analysis = _analyze_query_complexity_detailed(latest_query)
        complexity = complexity_analysis["complexity"]
        question_type = complexity_analysis["type"]
        urgency = complexity_analysis["urgency"]
        
        # FAQ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        faq_answer = _search_faq(latest_query)
        conversation_context = _build_conversation_context(history)
        
        # ìë™ ëª¨ë“œ ì„ íƒ
        selected_mode = _select_optimal_mode(complexity, question_type, urgency, bool(faq_answer))
        
        # ì„ íƒëœ ëª¨ë“œë¡œ ì‘ë‹µ ìƒì„±
        if selected_mode == "basic":
            response = _generate_basic_response(history)
        else:
            response = _generate_thinking_response_with_mode(history, selected_mode)
        
        return {
            "response": response,
            "selected_mode": selected_mode,
            "complexity": complexity,
            "question_type": question_type,
            "urgency": urgency,
            "quality_score": "enhanced" if selected_mode in ["thinking", "enhanced"] else "basic",
            "faq_matched": bool(faq_answer),
            "auto_selection": True,
            "analysis": {
                "length": complexity_analysis["length"],
                "indicators": complexity_analysis["indicators"]
            }
        }
        
    except Exception as e:
        print(f"[Advanced Response Error] {e}")
        return {
            "response": _generate_basic_response(history),
            "selected_mode": "basic",
            "complexity": "error",
            "quality_score": "fallback",
            "auto_selection": False,
            "error": str(e)
        }